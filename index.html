<!DOCTYPE html>
<html lang="en">
<head>
	<title>text Factory</title>
	<meta charset="utf-8"/>
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0"/>
	<meta name="description" content="thoughts and experiments in handmade code / generative text"/>
	<meta name="author" content="kathy mctavish">
	<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
	<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
	<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
	<link rel="manifest" href="/manifest.json">
	<script type="application/ld+json">
		{
			"@context": "http://schema.org",
			"@type": "WebPage",
			"name": "text Factory",
			"breadcrumb": "textfactory > text Factory",
          	"url": "https://textfactory.work/index.html",
			"description": "thoughts and experiments in handmade code / generative text",
			"datePublished": "Mon May 08 2023 17:08:04 GMT-0500 (Central Daylight Time)",
          	"image": "https://mctavish.work//apple-touch-icon.png",
			"author": "https://mctavish.studio/bio.html",
			"license": "http://creativecommons.org/licenses/by-nc-sa/3.0/us/deed.en.US"
		}
	</script>
	
	<!-- Google Analytics -->
	<link rel="stylesheet" href="/css/core.css"/>
	<script src="/code/core.js"></script>
	<style>
  		body {
			background: var(--warmblack);
		}
  		main {
			background: var(--warmblack);
		}
	</style>
</head><body id="top">
<div id="subtextframe" class="frame zlowest"></div>
<div id="svgframe" class="frame zlow"><svg xmlns="http://www.w3.org/2000/svg" id="svg" class="frame"></svg></div>
<div id="wordframe" class="frame z0"></div>
<div id="contentframe" class="absolute zhighest">

<div id="mainflex">
<main id="main">
<header>
	<h1>text Factory</h1>
	<h2>research on algorithmic text</h2>
</header>
<nav>
	<ul>
		<!-- <li><a href="#maincontent" id="skiptomaincontent">skip to main content</a></li> -->
		<li><a href="https://mctavish.io/index.html" id="homelink">go to mctavish portfolio</a></li>
	</ul>
</nav>
<div class="screenreader-text">
	<p>Your feedback is always welcome.</p>
</div>
<article>
	<header>
	<h1>intelligence</h1>
	</header>
	<p>
	knot intelligence hand intelligence weave intelligence organic intelligence photosynthetic intelligence tactile intelligence heart intelligence walking intelligence luminous half-closed eye subliminal intelligence
	local knowledge fleeting moment thoughts
	</p>
	<p>to know to no to immerse in the ecstatic electric torrent</p>
	<p>to become fodder chalk outline vacant stalk</p>
	</article>
	<article>
	<header>
	<h1>problematic physics</h1>
	</header>
	<p>
	opaque, innumerable, unknowable model parameters
	</p>
	<p>unexamined model dimensions not explicit not transparent not articulated expressed considered</p>
	<p>goals not confessed unexamined intentions implicit profit motives capitalist frameworks</p>
	<p>circular reasoning ::: model; training evaluation text spew becomes text input becomes confirmation</p>
	<p>vast data input becomes hyperlocal, averaged output</p>
	<p>
	averaging ::: we can all sound like the mean expression of slanted datasets
	</p>
	<p>these are hyper efficient colonization machines</p>
	<p>a confirmation bias dream machine ::: it is an endless, escher-like mirror of the uber-kings of data ::: show me what a tech worker looks like ... check! looks right to me!</p>
	<p>
		resources / effort / emphasis should be expended in creating self-documenting neural leaps / compression / distillation ... filters / assumptions / the shape of data that honed parameters ::: but this has no market value & there is very little regulating the application of these highly inaccurate, unpredictable tools on real-life problems that impact real people in real time
	</p>
	</article>
	<article>
	<header><h1>the environmental costs of LLMs</h1></header>
	<p>the resources used in chatgpt requests (let alone their original large-language model type training source) are astronomical .... we will definitely kill the earth by blithely engaging in frequent "chats." Although, search engines will increasingly use AI technology thereby making us all complicit in an exponential increase in energy costs for even our simple, curious search requests.</p>
	<p>we are in a LLM (Large Language Model) arms race between a small number of capitalist tech giants ::: sustainability of the planet isn't at the top of their list of priorities</p>
	<p>
		tech is often seen being as "resource-less" "objective" "cloud-like" "all-knowing" "pure" "outside the market" just thought science progress not dirty oil-burning, skewed, colonialist, ... this is the manhattan project ... a bunch of brilliant minds (white, priviledged men) working to push forward human understandiing of the world
	</p>
	<p>large language models are stochastic parrots, supercharged autocorrect .... hyper-powerful averaging machines</p>
	</article
	<article>
	<header><h1>role of an artist</h1><h2>dada the data</h2></header>
	<p>fodder ::: unpaid providers of honing advice, text, images, private data</p>
	<p>greenwash ::: make it seem cute ::: a toy</p>
	<p>
::: resist ::: flood the system ::: queer-up / dada the data
uber ze honey hive ::: electric phosynthetic queen ::: networked revolutionaries artists of the world :::
pollinate :::
</p>
<p>refuse ::: punk it up ::: create "bad text" "hand-hewn artifacts ... zines ... offline</p>
<p>do we need models trained on hundreds of billions of data points to create a poem about birds?</p>
<p>the tools we choose influence the work we make ::: cello, printing press, brush, pen, ...</p>
<p>we get to choose our tools ::: our hands ::: our bodies ::: our minds get to be a part of our creative process</p>
	</article>
	<article>
	<header><h1>references</h1><h2>more soon ...</h2></header>
	<p><a href="https://nmdprojects.net/teleconferences/nmd_webinar_ai_in_classroom_2023.html">AI in the classroom</a></p>
	<p><a href="https://www.theatlantic.com/technology/archive/2023/05/generative-ai-social-media-integration-dangers-disinformation-addiction/673940/">THe Atlantic: AI and toxic social media potential</a></p>
	<p><a href="https://umaine.edu/learnwithai/">University of Maine conversations about AI</a></p>
	<blockquote>
	<p>"Rather, it is built to maximize the extraction of wealth and profit – from both humans and the natural world – a reality that has brought us to what we might think of it as capitalism’s techno-necro stage."</p>
	<p><a href="https://www.theguardian.com/commentisfree/2023/may/08/ai-machines-hallucinating-naomi-klein">Klein, N. (2023, May 8). AI machines aren’t ‘hallucinating’. But their makers are.</a></p>
	</blockquote>
	<p><a href="https://www.numenta.com/blog/2022/05/24/ai-is-harming-our-planet/">energy use: a description of how it works</a></p>
	<blockquote>
	<p>"We need to take a step back and acknowledge that simply building ever-larger neural networks is not the right path to generalized intelligence. From first principles, we need to push ourselves to discover more elegant, efficient ways to model intelligence in machines. Our ongoing battle with climate change, and thus the future of our planet, depend on it."</p>
	<p><a href="https://www.forbes.com/sites/robtoews/2020/06/17/deep-learnings-climate-change-problem/?sh=5556d9a66b43">energy use link</a></p>
	</blockquote>
	<p><a href="https://www.govtech.com/question-of-the-day/how-much-water-does-chatgpt-drink-for-every-20-questions-it-answers">How much water does ChatGPT ‘drink’ for every 20 questions it answers?</a></p>
	<blockquote>
	<p>"he high cost of training and “inference” — actually running — large language models is a structural cost that differs from previous computing booms. Even when the software is built, or trained, it still requires a huge amount of computing power to run large language models because they do billions of calculations every time they return a response to a prompt. By comparison, serving web apps or pages requires much less calculation."</p>
	<p><a href="https://www.cnbc.com/2023/03/13/chatgpt-and-generative-ai-are-booming-but-at-a-very-expensive-price.html">cnbc: the cost of ai</a></p>
	</blockquote>
	<blockquote>
	<p>"To take just one example that’s very much in the news, ChatGPT-3—which we wrote about recently—has 175 billion machine learning (ML) parameters. It was trained on high-powered NVIDIA V100 graphical processing units (GPU), but researchers say it would have required 1,024 GPUs, 34 days and $4.6 million if done on A100 GPUs.
</p><p>
And while energy consumption was not officially disclosed, it is estimated that ChatGPT-3 consumed 936 MWh. That’s enough energy to power approximately 30,632 US households for one day, or 97,396 European households for the same period."
</p>
<p><a href="https://digitally.cognizant.com/ais-energy-use-isnt-sustainable-enter-tinyml-wf1584550">Cognizant blog</a></p>
	</blockquote>
	</article>
</main>
</div>
<footer>
<p><a href="#top" class="corelink">^ back to top</a></p>
</footer>
</div> <!-- end contentframe -->
</body>
</html>